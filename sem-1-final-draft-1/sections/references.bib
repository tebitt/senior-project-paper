@article {Palmer2024.07.17.24310551,
	author = {Palmer, Clare E and Marshall, Emily and Millgate, Edward and Warren, Graham and Ewbank, Michael P. and Cooper, Elisa and Lawes, Samantha and Bouazzaoui, Malika and Smith, Alastair and Hutchins-Joss, Chris and Young, Jessica and Margoum, Morad and Healey, Sandra and Marshall, Louise and Mehew, Shaun and Cummins, Ronan and Tablan, Valentin and Catarino, Ana and Welchman, Andrew E and Blackwell, Andrew D},
	title = {Combining AI and human support in mental health: a digital intervention with comparable effectiveness to human-delivered care},
	elocation-id = {2024.07.17.24310551},
	year = {2024},
	doi = {10.1101/2024.07.17.24310551},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {Escalating global mental health demand exceeds existing clinical capacity. Scalable digital solutions will be essential to expand access to high-quality mental healthcare. This study evaluated the effectiveness of a digital intervention to alleviate mild, moderate and severe symptoms of generalized anxiety. This structured, evidence-based program combined an Artificial Intelligence (AI) driven conversational agent to deliver content with human clinical oversight and user support to maximize engagement and effectiveness. The digital intervention was compared to three propensity-matched real-world patient comparator groups: i) waiting control; ii) face-to-face cognitive behavioral therapy (CBT); and iii) remote typed-CBT. Endpoints for effectiveness, engagement, acceptability, and safety were collected before, during and after the intervention, and at one-month follow-up. Participants (n=299) used the program for a median of 6 hours over 53 days. There was a large clinically meaningful reduction in anxiety symptoms for the intervention group (per-protocol (n=169): change on GAD-7 = -7.4, d = 1.6; intention-to-treat (n=299): change on GAD-7 = -5.4, d = 1.1) that was statistically superior to the waiting control, non-inferior to human-delivered care, and was sustained at one-month follow-up. By combining AI and human support, the digital intervention achieved clinical outcomes comparable to human-delivered care while significantly reducing the required clinician time. These findings highlight the immense potential of technology to scale effective evidence-based mental healthcare, address unmet need, and ultimately impact quality of life and economic burden globally.Competing Interest StatementChief Investigator (EMa) and other investigators (CEP, EMi, GW, MPE, EC, SL, AS, CH, JY, MB, LM, SM, RC, VT, AC, AW, AB) are employees of ieso Digital Health Limited (the company funding this research) or its subsidiaries. None of these authors had a direct financial incentive related to the results of this study or the publication of the manuscript.Clinical TrialISRCTN ID: 52546704Funding StatementThis research was funded by ieso Digital Health Ltd.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:NHS Research Ethics Committee (REC) West of Scotland REC 4 gave ethical approval for this research (IRAS ID: 327897)I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.YesOwing to the potential risk of patient identification, and following data privacy policies at ieso and DHC, individual-level data are not available. Aggregated data are available upon request, subject to a data-sharing agreement with ieso and DHC. Data requests should be sent to the corresponding author and will be responded to within 30 days.},
	URL = {https://www.medrxiv.org/content/early/2024/07/17/2024.07.17.24310551},
	eprint = {https://www.medrxiv.org/content/early/2024/07/17/2024.07.17.24310551.full.pdf},
	journal = {medRxiv}
}

@article{jarvis2024companion,
  title = {Jarvis the Companion},
  author = {Mamatha M and Abu Bakkar and Akash Hiremath and Amruthaiah and Jai Pranith},
  journal = {International Journal of Science and Research (IJSR)},
  volume = {13},
  number = {5},
  pages = {1672--1677},
  year = {2024},
  month = {May},
  issn = {2319-7064},
  doi = {10.21275/SR24508214759},
  url = {https://www.ijsr.net/},
  abstract = {The primary objective is to examine the capabilities of humanoid robots in providing both emotional support and executing tasks assigned to them. The humanoid robot, endowed with cutting-edge artificial intelligence and sensory technologies, adeptly replicates and responds to human emotions through realistic eye expressions and an integrated voice assistant system.}
}

@article{charles-2024,
	author = {Charles, John Thomas and Jannet, Sabitha and J, Clement Sudhahar},
	journal = {Research Square (Research Square)},
	month = {1},
	title = {{Mitigating Occupational Mental Health-Related factors to prevent manufacturing industry accidents}},
	year = {2024},
	doi = {10.21203/rs.3.rs-3771035/v1},
	url = {https://doi.org/10.21203/rs.3.rs-3771035/v1},
}

@article{GOH202372,
title = {Are patients with mental disorders more vulnerable to loneliness and social isolation during the COVID-19 pandemic? – Findings from the Japan COVID-19 and Society Internet Survey},
journal = {Journal of Affective Disorders},
volume = {340},
pages = {72-79},
year = {2023},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2023.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S0165032723010145},
author = {Keng Wee Goh and Shuhei Ishikawa and Ryo Okubo and Ichiro Kusumi and Takahiro Tabuchi},
keywords = {Mental disorders, Loneliness, Social isolation, COVID-19 pandemic, Depression, Anxiety},
abstract = {Background
Loneliness and social isolation are well-known factors that worsen the symptoms among patients with mental disorders. Few previous studies have explored loneliness and social isolation among populations with mental disorders during the coronavirus disease 2019 (COVID-19) pandemic. Therefore, our study examined the mental health impact of the pandemic on these population groups in terms of loneliness and social isolation.
Methods
We used data from the Japan COVID-19 and Society Internet Surveys, a large-scale online survey. Using multivariable logistic regression analysis, we calculated the odds ratios and 95 % confidence intervals (CIs) of moderate-to-severe loneliness and high social isolation for major chronic diseases, including mental disorders, after adjusting for potential confounders. Calculations were performed for each type of mental disorder. Finally, calculations were performed to explore the association between moderate-to-severe loneliness or high social isolation and psychiatric symptoms among patients with mental disorders.
Results
Of the 28,175 participants, 2021 (7.2 %) had a mental disorder. Mental disorders, especially depression and anxiety disorders, were found to be associated with a higher risk of moderate-to-severe loneliness and high social isolation. Patients with mental disorders who experienced moderate-to-severe loneliness and high social isolation were found to have exacerbated psychiatric symptoms.
Limitation
Our findings were obtained from a cross-sectional study design.
Conclusions
Patients with mental disorders were more vulnerable to moderate-to-severe loneliness and high social isolation during the pandemic, which contributed to the exacerbation of their symptoms. Depression and anxiety, in particular, were most likely to occur and required special attention.}
}

@article{Sukhawathanakul_Crizzle_Tuokko_Naglie_Rapoport_2021, place={Markham, Canada}, title={Psychotherapeutic Interventions for Dementia: a Systematic Review}, volume={24}, url={https://cgjonline.ca/index.php/cgj/article/view/447}, DOI={10.5770/cgj.24.447}, abstractNote={&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Background and Objectives &amp;lt;/strong&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;While a range of psychotherapeutic interventions is available to support individuals with dementia, comprehensive reviews of interventions are limited, particularly with regard to outcomes related to adjustment and acceptance. The current review assesses studies that evaluated the impact of various forms of psychotherapeutic interventions on acceptance and adjustment to changing life circumstances for older adults with cognitive impairment.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Research Design and Methods &amp;lt;/strong&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;A systematic search of PubMed, PsycINFO, and CINAHL databases was conducted, restricted to articles published in English within the last 16 years (from 2003 to 2019). Twenty-four articles were identified that examined the effects of psychotherapeutic interventions on outcomes related to acceptance and adjustment which included internalizing symptoms, quality of life, self-esteem, and well-being. Fifteen studies examined interventions targeted towards individuals with cognitive impairment, while nine studies also targeted their caregivers.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Results &amp;lt;/strong&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Interventions that impacted outcomes related to acceptance and adjustment (e.g., adaptation, depressive symptoms, helplessness, self-esteem, and quality of life) varied in their theoretical approach, which incorporated elements of cognitive behavioural therapy (CBT), problem-solving therapy, psychotherapy, reminiscence therapy, interpersonal therapy, mindfulness-based therapy, and meaning-based, compassion-focused therapy. Among all interventions, reductions in depression were the most commonly reported treatment outcome particularly among interventions that&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;amp;nbsp;incorporated problem-focused and meaning-based therapies. Mixed findings were reported with regard to outcomes related to helplessness, quality of life, self-esteem, and life satisfaction indices for individuals with cognitive impairment.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Discussion and Implications &amp;lt;/strong&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;There is some support for the effectiveness of psychotherapeutic interventions on improving acceptance and adjustment in older adults with cognitive impairment, particularly with regard to reducing depressive symptoms.&amp;lt;/p&amp;gt;}, number={3}, journal={Canadian Geriatrics Journal}, author={Sukhawathanakul, Paweena and Crizzle, Alexander and Tuokko, Holly and Naglie, Gary and Rapoport, Mark J.}, year={2021}, month={Sep.}, pages={222–236} }

@book{nia2023,
  author    = {{National Innovation Agency [NIA]}},
  title     = {Futures of Mental Health in Thailand in 2033},
  year      = {2023},
  publisher = {NIA Thailand},
  url       = {https://www.nia.or.th/bookshelf/view/237},
} 

@inproceedings{10.1145/3640794.3665551,
author = {De Cet, Martina and Cvajner, Martina and Torre, Ilaria and Obaid, Mohammad},
title = {Do Your Expectations Match? A Mixed-Methods Study on the Association Between a Robot's Voice and Appearance},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665551},
doi = {10.1145/3640794.3665551},
abstract = {Both physical appearance and voice can elicit mental images of what someone and/or something should sound and look like. This is particularly relevant for human-robot interaction design and research since any voice can be added to a robot. Therefore, it is important to give robots voices that match users’ expectations. In this paper, we examined the voice-appearance association by asking participants to match a robot image with a voice (Experiment 1, N = 24), and vice versa, a voice with a robot image (Experiment 2, N = 24), in two mixed-methods studies. We looked at participants’ differences that could influence the voice-robot association (gender and nationality) and at voice and robot features that could influence participants’ voice preferences (voice gender, pitch and robot’s appearance). Results show that nationality influenced participants’ association with a robot image after hearing its voice. Furthermore, a content analysis identified that when creating a voice mental image, participants looked at robots’ gendered characteristics and height and they paid special attention to human-like and gender-specific cues in a voice when forming a mental image of a robot. Sociological differences also emerged, with Swedish participants suggesting the use of gender-neutral voices to avoid strengthening existing stereotypes, and Italians saying the opposite. Our work highlights the importance of individual differences in the robot voice-appearance association and the importance of involving the end user in designing the voice.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {21},
numpages = {11},
keywords = {Agent, Appearance, Robot, Voice},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@article{Wang_2024, title={Enhancing User Experience Using a Framework Integrating Emotion Recognition and Eye-Tracking}, volume={85}, url={https://drpress.org/ojs/index.php/HSET/article/view/18387}, DOI={10.54097/be97jg10}, abstractNote={
In a rapidly evolving digital landscape, ensuring a seamless and engaging user experience (UX) has become paramount. This study delves into the intricate realm of web interaction design, aiming to enhance user satisfaction and engagement. Through the integration of emotion recognition and eye-tracking technologies, a profound relationship between user emotions and web interface design is unveiled. The central theme of this research revolves around the integration of emotion recognition and eye-tracking technologies to evaluate web interaction designs. An experimental study involving 24 participants from diverse backgrounds and age groups was conducted. These participants navigated web interfaces that featured both emotion recognition and eye-tracking technologies. Three distinct tasks were carefully crafted to represent a spectrum of web interactions, ranging from basic navigation to complex decision-making. Data collection involved real-time valence and arousal scores, video recordings, visual attention patterns, task completion times, and questionnaires. The analysis revealed a series of significant discoveries. Users who engaged with simplified web versions consistently exhibited elevated valence values, indicative of heightened positive emotional feedback, diverging starkly from their counterparts navigating complex web iterations. The dynamic ebb and flow of emotional states, underscored by arousal levels, underscore the pivotal role of real-time emotional assessment in the critical evaluation of web interfaces. Moreover, the study unveiled a persistent preference for a state of emotional calmness during interactions, demonstrating a universal need for user-centered design principles that prioritize minimal cognitive load and emotional tranquility. In summation, this research contributes a robust framework to the design community and academia for the comprehensive evaluation of web interaction designs. The findings underscore the paramount significance of simplicity, real-time emotional evaluation, and unwavering adherence to user-centered design principles in the realm of web interaction. This study constitutes an invaluable repository for designers, developers, and researchers, steering their endeavors towards the relentless pursuit of optimized user experiences within the ever-evolving digital landscape.
}, journal={Highlights in Science, Engineering and Technology}, author={Wang, Jianyi}, year={2024}, month={Mar.}, pages={298–308} }

@misc{lillo2024investigatingrelationshipempathyattribution,
      title={Investigating the relationship between empathy and attribution of mental states to robots}, 
      author={Alberto Lillo and Alessandro Saracco and Elena Siletto and Claudio Mattutino and Cristina Gena},
      year={2024},
      eprint={2405.01019},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2405.01019}, 
}

@article{HAKANPAA2021570,
title = {Comparing Contemporary Commercial and Classical Styles: Emotion Expression in Singing},
journal = {Journal of Voice},
volume = {35},
number = {4},
pages = {570-580},
year = {2021},
issn = {0892-1997},
doi = {https://doi.org/10.1016/j.jvoice.2019.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0892199719302097},
author = {Tua Hakanpää and Teija Waaramaa and Anne-Maria Laukkanen},
keywords = {Emotion expression, Singing voice, Voice quality, Song genre, Acoustic analyses},
abstract = {Summary
Objective
This study examines the acoustic correlates of the vocal expression of emotions in contemporary commercial music (CCM) and classical styles of singing. This information may be useful in improving the training of interpretation in singing.
Study Design
This is an experimental comparative study.
Methods
Eleven female singers with a minimum of 3 years of professional-level singing training in CCM, classical, or both styles participated. They sang the vowel [ɑ:] at three pitches (A3 220Hz, E4 330Hz, and A4 440Hz) expressing anger, sadness, joy, tenderness, and a neutral voice. Vowel samples were analyzed for fundamental frequency (fo) formant frequencies (F1-F5), sound pressure level (SPL), spectral structure (alpha ratio = SPL 1500-5000 Hz—SPL 50-1500 Hz), harmonics-to-noise ratio (HNR), perturbation (jitter, shimmer), onset and offset duration, sustain time, rate and extent of fo variation in vibrato, and rate and extent of amplitude vibrato.
Results
The parameters that were statistically significantly (RM-ANOVA, P ≤ 0.05) related to emotion expression in both genres were SPL, alpha ratio, F1, and HNR. Additionally, for CCM, significance was found in sustain time, jitter, shimmer, F2, and F4. When fo and SPL were set as covariates in the variance analysis, jitter, HNR, and F4 did not show pure dependence on expression. The alpha ratio, F1, F2, shimmer apq5, amplitude vibrato rate, and sustain time of vocalizations had emotion-related variation also independent of fo and SPL in the CCM style, while these parameters were related to fo and SPL in the classical style.
Conclusions
The results differed somewhat for the CCM and classical styles. The alpha ratio showed less variation in the classical style, most likely reflecting the demand for a more stable voice source quality. The alpha ratio, F1, F2, shimmer, amplitude vibrato rate, and the sustain time of the vocalizations were related to fo and SPL control in the classical style. The only common independent sound parameter indicating emotional expression for both styles was SPL. The CCM style offers more freedom for expression-related changes in voice quality.}
}

@Article{app8122629,
  AUTHOR = {Wang, Ting and Lee, Yong-cheol and Ma, Qiuwu},
  TITLE = {Within and Across-Language Comparison of Vocal Emotions in Mandarin and English},
  JOURNAL = {Applied Sciences},
  VOLUME = {8},
  YEAR = {2018},
  NUMBER = {12},
  ARTICLE-NUMBER = {2629},
  URL = {https://www.mdpi.com/2076-3417/8/12/2629},
  ISSN = {2076-3417},
  ABSTRACT = {This study reports experimental results on whether the acoustic realization of vocal emotions differs between Mandarin and English. Prosodic cues, spectral cues and articulatory cues generated by electroglottograph (EGG) of five emotions (anger, fear, happiness, sadness and neutral) were compared within and across Mandarin and English through a production experiment. Results of within-language comparison demonstrated that each vocal emotion had specific acoustic patterns in each language. Moreover, normalized data were used in the across-language comparison analysis. Results indicated that Mandarin and English showed different mechanisms of utilizing pitch for encoding emotions. The differences in pitch variation between neutral and other emotions were significantly larger in English than in Mandarin. However, the variations of speech rate and certain phonation cues (e.g., CPP (Cepstral Peak Prominence) and CQ (Contact quotient)) were significantly greater in Mandarin than in English. The differences in emotional speech between the two languages may be due to the restriction of pitch variation by the presence of lexical tones in Mandarin. This study reveals an interesting finding that occurs when a certain cue (e.g., pitch) is restricted in one language, other cues were strengthened to take on the responsibility of differentiating vocal emotions. Therefore, we posit that the acoustic realizations of emotional speech are multidimensional.},
  DOI = {10.3390/app8122629}
}

@article{RODERO2011e25,
  title = {Intonation and Emotion: Influence of Pitch Levels and Contour Type on Creating Emotions},
  journal = {Journal of Voice},
  volume = {25},
  number = {1},
  pages = {e25-e34},
  year = {2011},
  issn = {0892-1997},
  doi = {https://doi.org/10.1016/j.jvoice.2010.02.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0892199710000378},
  author = {Emma Rodero},
  keywords = {Emotion, Intonation, Pitch level, Contour type},
  abstract = {Summary
  Intonation is a vehicle for communication, which sometimes contributes greater meaning than the semantic content of speech itself. This prosodic element lends the message linguistic and paralinguistic meaning, which carries a highly significant communicative value when conveying emotional states. For this reason, this article analyses the use of intonation as an instrument for arousing various sensations in the listener. The aim was to verify which elements of intonation are more decisive to generate a specific sensation. Experimental research is conducted, in which certain pitch patterns (pitch levels and contour type) are assigned different emotions (joy, anxiety, sadness, and calmness) and are then listened to and assessed using a questionnaire with a bipolar scale of opposed pairs, by a sample audience comprising 100 individuals. The main conclusion drawn is that, although both the variables analyzed—pitch level and contour type—are representative of expressing emotions, contour type is more decisive. In all the models analyzed, contour type has been highly significant and constitutes the variable that has been determined as the final component for recognizing various emotions.}
}


@article{abbas2024context,
  title = {Context-based Emotion Recognition: A Survey},
  author = {Rizwan Abbas and Bingnan Ni and Ruhui Ma and Teng Li and Yehao Lu and Xi Li},
  journal = {Neurocomputing},
  year = {2024},
  keywords = {emotion recognition, contextual information, emotion detection techniques, real-life applications},
  note = {Preprint submitted to Elsevier, not peer-reviewed},
  url = {https://ssrn.com/abstract=4657124},
  abstract = {Emotions are essential to human communication, and their accurate recognition is crucial for developing intelligent systems capable of effective interaction. This survey explores the critical role of context in emotion recognition, examining techniques, datasets, and applications across fields. It highlights challenges and identifies future directions for advancement in this area.},
}

@article{chapre2023emotion,
  title = {Emotion / Facial Expression Detection},
  author = {Harshada Chapre and Lopita Choudhury and Manoj Kamber},
  journal = {International Journal of Science and Research (IJSR)},
  volume = {12},
  number = {5},
  pages = {1395--1398},
  year = {2023},
  month = {May},
  issn = {2319-7064},
  doi = {10.21275/SR23516180518},
  url = {https://www.ijsr.net/},
  abstract = {Facial Expression conveys non-verbal cues, playing an essential role in interpersonal relations. This paper discusses a Facial Expression Recognition system that compares captured images with a trained dataset to identify emotional states. Techniques include Local Binary Pattern (LBP) and Support Vector Machine (SVM).},
}

@misc{liu2024emotiondetectionbodygesture,
      title={Emotion Detection through Body Gesture and Face}, 
      author={Haoyang Liu},
      year={2024},
      eprint={2407.09913},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.09913}, 
}


@Article{computation11030052,
AUTHOR = {Taye, Mohammad Mustafa},
TITLE = {Theoretical Understanding of Convolutional Neural Network: Concepts, Architectures, Applications, Future Directions},
JOURNAL = {Computation},
VOLUME = {11},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2079-3197/11/3/52},
ISSN = {2079-3197},
ABSTRACT = {Convolutional neural networks (CNNs) are one of the main types of neural networks used for image recognition and classification. CNNs have several uses, some of which are object recognition, image processing, computer vision, and face recognition. Input for convolutional neural networks is provided through images. Convolutional neural networks are used to automatically learn a hierarchy of features that can then be utilized for classification, as opposed to manually creating features. In achieving this, a hierarchy of feature maps is constructed by iteratively convolving the input image with learned filters. Because of the hierarchical method, higher layers can learn more intricate features that are also distortion and translation invariant. The main goals of this study are to help academics understand where there are research gaps and to talk in-depth about CNN’s building blocks, their roles, and other vital issues.},
DOI = {10.3390/computation11030052}
}

@article{gupta-2024,
	author = {Gupta, Sneha},
	journal = {International Journal for Research in Applied Science and Engineering Technology},
	month = {4},
	number = {4},
	pages = {71--74},
	title = {{Enhancing Mood Based Music Selection through Physiological Sensing Technology}},
	volume = {12},
	year = {2024},
	doi = {10.22214/ijraset.2024.59658},
	url = {https://doi.org/10.22214/ijraset.2024.59658},
}

@article{10.48175/ijarsct-15385,
  author = {Chethan C V and Greeshma K S and Dr. Kiran Y C},
  title = {Emotion detection via voice and speech recognition},
  journal = {International Journal of Advanced Research in Science, Communication and Technology},
  year = {2024},
  pages = {635-643},
  doi = {10.48175/ijarsct-15385}
}

@INPROCEEDINGS{1642310,
  author={Wada, K. and Shibata, T.},
  booktitle={Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.}, 
  title={Robot therapy in a care house - its sociopsychological and physiological effects on the residents}, 
  year={2006},
  volume={},
  number={},
  pages={3966-3971},
  keywords={Medical treatment;Senior citizens;Animals;Social network services;Service robots;Seals;Psychology;Testing;Humans;Hospitals},
  doi={10.1109/ROBOT.2006.1642310}}

@misc{hurst2020socialemotionalskillstraining,
  title={Social and Emotional Skills Training with Embodied Moxie}, 
  author={Nikki Hurst and Caitlyn Clabaugh and Rachel Baynes and Jeff Cohn and Donna Mitroff and Stefan Scherer},
  year={2020},
  eprint={2004.12962},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2004.12962}, 
}

@inproceedings{baltrusaitis2018,
  author    = {Tadas Baltrušaitis and Amir Zadeh and Yao Chong Lim and Louis-Philippe Morency},
  title     = {OpenFace 2.0: Facial Behavior Analysis Toolkit},
  booktitle = {2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018)},
  year      = {2018},
  pages     = {59-66},
  doi       = {10.1109/FG.2018.00019},
  url       = {https://doi.org/10.1109/FG.2018.00019},
  publisher = {IEEE},
}

@inproceedings{zhang2023,
  author    = {Ran Zhang},
  title     = {Facial Emotion Detection Based on Improved VGG-16},
  booktitle = {Proceedings of the 5th International Conference on Computing and Data Science},
  year      = {2023},
  doi       = {10.54254/2755-2721/21/20231110},
  url       = {https://doi.org/10.54254/2755-2721/21/20231110},
  publisher = {Open Access Proceedings},
}

@article{swethashree2021,
  author    = {Swethashree A. and Ganesh T. and J. Aravind and M. Venkatratna and R. Gayathri},
  title     = {Speech Emotion Recognition},
  journal   = {International Journal for Research in Applied Science \& Engineering Technology (IJRASET)},
  volume    = {9},
  number    = {VIII},
  pages     = {2637-2640},
  year      = {2021},
  doi       = {10.22214/ijraset.2021.37375},
  url       = {https://doi.org/10.22214/ijraset.2021.37375},
}

@article{devlin2019,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal   = {arXiv preprint arXiv:1810.04805},
  year      = {2019},
  doi       = {10.48550/arXiv.1810.04805},
  url       = {https://doi.org/10.48550/arXiv.1810.04805},
}

@article{shahriar2024,
  author    = {Sakib Shahriar and Brady D. Lund and Nishith Reddy Mannuru and Muhammad Arbab Arshad and Kadhim Hayawi and Ravi Varma Kumar Bevara and Aashrith Mannuru and Laiba Batool},
  title     = {Putting GPT-4o to the Sword: A Comprehensive Evaluation of Language, Vision, Speech, and Multimodal Proficiency},
  journal   = {Preprints},
  year      = {2024},
  doi       = {10.20944/preprints202406.1635.v1},
  url       = {https://doi.org/10.20944/preprints202406.1635.v1},
}

@inproceedings{wu2024,
  author    = {Yichao Wu and Zhengyu Jin and Chenxi Shi and Penghao Liang and Tong Zhan},
  title     = {Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis},
  booktitle = {Proceedings of the 2nd International Conference on Software Engineering and Machine Learning},
  year      = {2024},
  doi       = {10.54254/2755-2721/67/2024MA},
  url       = {https://doi.org/10.54254/2755-2721/67/2024MA},
}

@article{abuhmida2024,
  author    = {Mabrouka Abuhmida and Md Johirul Islam and Wendy Booth},
  title     = {Empathy in AI: Developing a Sentiment-Sensitive Chatbot through Advanced Natural Language Processing},
  journal   = {International Journal of Advanced Trends in Computer Science and Engineering},
  volume    = {13},
  number    = {3},
  pages     = {140-147},
  year      = {2024},
  doi       = {10.54254/2755-2721/67/2024MA},
  url       = {https://doi.org/10.54254/2755-2721/67/2024MA},
}

@article{safavi2024,
  author    = {Farshad Safavi and Parthan Olikkal and Dingyi Pei and Sadia Kamal and Helen Meyerson and Varsha Penumalee and Ramana Vinjamuri},
  title     = {Emerging Frontiers in Human–Robot Interaction},
  journal   = {Journal of Intelligent \& Robotic Systems},
  volume    = {110},
  number    = {45},
  year      = {2024},
  doi       = {10.1007/s10846-024-02074-7},
  url       = {https://doi.org/10.1007/s10846-024-02074-7},
}

@article{han-2023,
	author = {Han, Shangfeng and Guo, Yanliang and Zhou, Xinyi and Huang, Junlong and Shen, Linlin and Luo, Yuejia},
	journal = {Scientific Data},
	month = {12},
	number = {1},
	title = {{A Chinese Face Dataset with Dynamic Expressions and Diverse Ages Synthesized by Deep Learning}},
	volume = {10},
	year = {2023},
	doi = {10.1038/s41597-023-02701-2},
	url = {https://doi.org/10.1038/s41597-023-02701-2},
}

@misc{simonyan2015deepconvolutionalnetworkslargescale,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.1556}, 
}

@misc{hinka_rilakkuma_history,
  author = {{Hinka}},
  title = {The History of Rilakkuma and Friends: From Stress Buster to Kawaii Stationery},
  url = {https://www.hinka.co/blog/the-history-of-rilakkuma-and-friends-from-stress-buster-to-kawaii-stationery},
  note = {Accessed: 2024-11-22}
}
