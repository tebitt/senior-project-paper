\section{Research Background}
This section will provide justifications for the project and necessary knowledge for the reader in order to
understand technical terms used throughout the report.

\subsection{Justification of the Project}

The project focuses on developing emotional well-being robots that promote mental wellness and positivity for users under the influence of stress and anxiety. In general, emotion-related robots are designed to respond to human emotions and can potentially achieve clinical outcomes similar to traditional therapy \cite{Palmer2024.07.17.24310551}. Research has shown that digital interventions, such as AI-powered mental well-being robots, can effectively reduce anxiety symptoms and address unmet mental health needs, offering a promising solution to supplement traditional therapeutic approaches \cite{jarvis2024companion}.

The mental health industry faces significant challenges that cannot be fully addressed through human intervention alone \cite{charles-2024}. Key issues include loneliness and social isolation, which are major contributors to depression, anxiety, and overall deterioration in mental health \cite{GOH202372}, as well as therapeutic challenges, where patients with dementia or other cognitive impairments often struggle with traditional therapeutic activities \cite{Sukhawathanakul_Crizzle_Tuokko_Naglie_Rapoport_2021}. In a study by the National Innovation Agency (NIA) based in Thailand, it was identified that the concept of \textbf{\textit{Terror Outbursts}} would become a pressing issue in Thailand by the year 2033 \cite{nia2023}. To elaborate, terror outburst refers to a society driven by constant fear and anxiety. Consequently, traditional methods of addressing anxiety, such as therapy and medication, may not be accessible or appealing to everyone. This creates a significant pain point for individuals seeking immediate, non-invasive support. Our target customer segment includes young adults and professionals aged 18-35 who experience mild to moderate anxiety but may be hesitant to seek conventional treatment, where we provide an innovative alternative to support their mental well-being.

To ensure emotional well-being robots meet user needs and deliver effective support, three key pillars are essential: appearance, interactivity, and empathy. First, the robot’s appearance should strike a balance between human-like and machine-like traits, fostering both comfort and trust in users \cite{10.1145/3640794.3665551}. High interactivity is also crucial; the robot should provide adaptive feedback through various stimuli to engage users effectively and enhance their emotional states \cite{Wang_2024}. Moreover, a robot’s perceived empathic abilities play a significant role in how users interact with it, as these perceptions directly influence their willingness to attribute mental states to the robot, thereby impacting the overall quality of the interaction \cite{lillo2024investigatingrelationshipempathyattribution}. On the concept of emotion detection, traditional emotional detection methods utilize verbal and non-verbal cues to accurately detect and respond to human emotions. Verbal cues like pitch variations, volume, and speech rate \cite{HAKANPAA2021570} are critical indicators of emotional states. For example, higher pitch and increased volume often signal heightened emotional arousal, as seen in both American English and Mandarin Chinese, where pitch and speed are essential for expressing emotions. Additionally, contextual understanding—interpreting emotions based on situational cues—further refines the robot’s emotional recognition capabilities \cite{abbas2024context}. Non-verbal cues, such as facial expressions and body language, also play a vital role. For instance, a smile usually denotes happiness, while crossed arms might suggest defensiveness \cite{liu2024emotiondetectionbodygesture}. By integrating these verbal and non-verbal indicators, mental well-being support robots can offer tailored responses, thereby improving the overall effectiveness of their interactions with users.

\subsection{Necessary Knowledge}

The development of a mental well-being support robot with emotional detection capabilities requires a strong foundation in various advanced concepts within artificial intelligence, machine learning, robotics, and human-computer interaction. Below is an overview of the essential knowledge areas for this project:

Machine learning models are the backbone of emotion detection systems. Convolutional Neural Networks (CNNs) \cite{computation11030052} are widely used for tasks such as facial emotion recognition, where they excel at analyzing image data to identify patterns corresponding to different emotional states. One specific architecture, VGGNet, has proven effective for emotion detection due to its deep, layered structure and ability to capture fine-grained facial features. VGGNet's simplicity in design \cite{computation11030052}, using smaller 3x3 filters stacked in depth, makes it particularly useful for recognizing subtle facial expressions that correspond to emotions. This capability enhances the accuracy of emotion detection, which is crucial for the mental well-being support robot to respond appropriately to a user's emotional state.

In the visual domain, key facial features like eyes, mouth, and eyebrows are extracted and analyzed by CNNs to detect emotions from facial expressions. However, effective emotion recognition often requires the consideration of temporal patterns in sequences of images, such as micro-expressions that unfold over time. Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) \cite{schmidt2019} networks are essential for processing such sequential data. LSTMs, in particular, are highly effective at retaining information over extended time periods, enabling the robot to identify subtle changes in facial expressions or gestures that might otherwise go unnoticed.

Speech recognition is crucial for enabling the robot to understand and interpret human speech, which is key to detecting emotions from spoken input. Speech recognition techniques allow the robot to process audio data, extracting meaningful insights such as tone, pitch, and speech patterns. These insights help the robot assess the emotional tone and context of the user’s communication, making it possible to respond appropriately to their emotional needs. Similar to visual emotion recognition, LSTMs are also invaluable in analyzing sequential audio features, ensuring that variations in tone or pitch over time are captured effectively.

Effective emotion detection relies on extracting meaningful features from raw data. For instance, Mel-Frequency Cepstral Coefficients (MFCCs) \cite{singh2014} are a crucial feature extraction technique in speech emotion recognition, capturing the essential characteristics of the audio signal that correlate with emotional states. Similarly, in the visual domain, CNNs extract and analyze key facial features like eyes, mouth, and eyebrows to detect emotions from facial expressions. The combination of CNNs for spatial analysis and LSTMs for temporal analysis creates a robust framework for identifying emotions from both static and dynamic data.

The design of emotionally intelligent robots requires an understanding of Human-Robot Interaction (HRI) principles. These principles guide the development of robots that can interact naturally and empathetically with humans. Concepts such as user-friendly interface design, adaptive behavior, and empathetic response mechanisms ensure that the robot’s interactions are socially acceptable and supportive.

Additionally, emotionally intelligent robots rely on a combination of advanced hardware and software to accurately detect and respond to human emotions. Key hardware components, including cameras, are essential for capturing detailed facial expressions in real-time, allowing systems to effectively analyze emotional states \cite{gupta-2024}. Microphones and audio sensors play a crucial role in gathering vocal cues, which are vital for emotion detection \cite{10.48175/ijarsct-15385}. Processors and GPUs manage the heavy computational tasks, while actuators and motors control the robot’s physical movements, such as gestures and facial expressions, enabling the robot to convey empathy and respond to users effectively. Haptic sensors further enhance this interaction by reacting to touch, contributing to a more interactive and supportive user experience.

Lastly, Bayesian Networks provide a robust framework for decision-making \cite{DBLP:journals/corr/abs-2002-00269}, enabling the robot to infer emotional states and choose appropriate responses. These graphical models represent variables and their dependencies through directed acyclic graphs (DAGs). For the robot, observable inputs like facial expressions, vocal cues, and contextual data are nodes, while hidden nodes represent inferred emotional states such as sadness or anxiety.

Bayesian Networks allow the robot to integrate prior knowledge and update beliefs with new information. A belief represents the robot's degree of confidence in a particular state or outcome, based on available evidence and prior knowledge. For example, if vocal cues indicate frustration but facial expressions appear neutral, the network can combine these inputs to infer the true emotional state. This approach assists the robot in making informed decisions and avoiding ambiguity or conflicting signals in data.