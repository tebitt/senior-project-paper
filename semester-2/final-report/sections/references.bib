@article {Palmer2024.07.17.24310551,
	author = {Palmer, Clare E and Marshall, Emily and Millgate, Edward and Warren, Graham and Ewbank, Michael P. and Cooper, Elisa and Lawes, Samantha and Bouazzaoui, Malika and Smith, Alastair and Hutchins-Joss, Chris and Young, Jessica and Margoum, Morad and Healey, Sandra and Marshall, Louise and Mehew, Shaun and Cummins, Ronan and Tablan, Valentin and Catarino, Ana and Welchman, Andrew E and Blackwell, Andrew D},
	title = {Combining AI and human support in mental health: a digital intervention with comparable effectiveness to human-delivered care},
	elocation-id = {2024.07.17.24310551},
	year = {2024},
	doi = {10.1101/2024.07.17.24310551},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {Escalating global mental health demand exceeds existing clinical capacity. Scalable digital solutions will be essential to expand access to high-quality mental healthcare. This study evaluated the effectiveness of a digital intervention to alleviate mild, moderate and severe symptoms of generalized anxiety. This structured, evidence-based program combined an Artificial Intelligence (AI) driven conversational agent to deliver content with human clinical oversight and user support to maximize engagement and effectiveness. The digital intervention was compared to three propensity-matched real-world patient comparator groups: i) waiting control; ii) face-to-face cognitive behavioral therapy (CBT); and iii) remote typed-CBT. Endpoints for effectiveness, engagement, acceptability, and safety were collected before, during and after the intervention, and at one-month follow-up. Participants (n=299) used the program for a median of 6 hours over 53 days. There was a large clinically meaningful reduction in anxiety symptoms for the intervention group (per-protocol (n=169): change on GAD-7 = -7.4, d = 1.6; intention-to-treat (n=299): change on GAD-7 = -5.4, d = 1.1) that was statistically superior to the waiting control, non-inferior to human-delivered care, and was sustained at one-month follow-up. By combining AI and human support, the digital intervention achieved clinical outcomes comparable to human-delivered care while significantly reducing the required clinician time. These findings highlight the immense potential of technology to scale effective evidence-based mental healthcare, address unmet need, and ultimately impact quality of life and economic burden globally.Competing Interest StatementChief Investigator (EMa) and other investigators (CEP, EMi, GW, MPE, EC, SL, AS, CH, JY, MB, LM, SM, RC, VT, AC, AW, AB) are employees of ieso Digital Health Limited (the company funding this research) or its subsidiaries. None of these authors had a direct financial incentive related to the results of this study or the publication of the manuscript.Clinical TrialISRCTN ID: 52546704Funding StatementThis research was funded by ieso Digital Health Ltd.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:NHS Research Ethics Committee (REC) West of Scotland REC 4 gave ethical approval for this research (IRAS ID: 327897)I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.YesOwing to the potential risk of patient identification, and following data privacy policies at ieso and DHC, individual-level data are not available. Aggregated data are available upon request, subject to a data-sharing agreement with ieso and DHC. Data requests should be sent to the corresponding author and will be responded to within 30 days.},
	URL = {https://www.medrxiv.org/content/early/2024/07/17/2024.07.17.24310551},
	eprint = {https://www.medrxiv.org/content/early/2024/07/17/2024.07.17.24310551.full.pdf},
	journal = {medRxiv}
}

@article{jarvis2024companion,
  title = {Jarvis the Companion},
  author = {Mamatha M and Abu Bakkar and Akash Hiremath and Amruthaiah and Jai Pranith},
  journal = {International Journal of Science and Research (IJSR)},
  volume = {13},
  number = {5},
  pages = {1672--1677},
  year = {2024},
  month = {May},
  issn = {2319-7064},
  doi = {10.21275/SR24508214759},
  url = {https://www.ijsr.net/},
  abstract = {The primary objective is to examine the capabilities of humanoid robots in providing both emotional support and executing tasks assigned to them. The humanoid robot, endowed with cutting-edge artificial intelligence and sensory technologies, adeptly replicates and responds to human emotions through realistic eye expressions and an integrated voice assistant system.}
}

@article{charles-2024,
	author = {Charles, John Thomas and Jannet, Sabitha and J, Clement Sudhahar},
	journal = {Research Square (Research Square)},
	month = {1},
	title = {{Mitigating Occupational Mental Health-Related factors to prevent manufacturing industry accidents}},
	year = {2024},
	doi = {10.21203/rs.3.rs-3771035/v1},
	url = {https://doi.org/10.21203/rs.3.rs-3771035/v1},
}

@book{nia2023,
  author    = {{National Innovation Agency [NIA]}},
  title     = {Futures of Mental Health in Thailand in 2033},
  year      = {2023},
  publisher = {NIA Thailand},
  url       = {https://www.nia.or.th/bookshelf/view/237},
} 

@inproceedings{10.1145/3640794.3665551,
author = {De Cet, Martina and Cvajner, Martina and Torre, Ilaria and Obaid, Mohammad},
title = {Do Your Expectations Match? A Mixed-Methods Study on the Association Between a Robot's Voice and Appearance},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665551},
doi = {10.1145/3640794.3665551},
abstract = {Both physical appearance and voice can elicit mental images of what someone and/or something should sound and look like. This is particularly relevant for human-robot interaction design and research since any voice can be added to a robot. Therefore, it is important to give robots voices that match users’ expectations. In this paper, we examined the voice-appearance association by asking participants to match a robot image with a voice (Experiment 1, N = 24), and vice versa, a voice with a robot image (Experiment 2, N = 24), in two mixed-methods studies. We looked at participants’ differences that could influence the voice-robot association (gender and nationality) and at voice and robot features that could influence participants’ voice preferences (voice gender, pitch and robot’s appearance). Results show that nationality influenced participants’ association with a robot image after hearing its voice. Furthermore, a content analysis identified that when creating a voice mental image, participants looked at robots’ gendered characteristics and height and they paid special attention to human-like and gender-specific cues in a voice when forming a mental image of a robot. Sociological differences also emerged, with Swedish participants suggesting the use of gender-neutral voices to avoid strengthening existing stereotypes, and Italians saying the opposite. Our work highlights the importance of individual differences in the robot voice-appearance association and the importance of involving the end user in designing the voice.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {21},
numpages = {11},
keywords = {Agent, Appearance, Robot, Voice},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@article{Wang_2024, title={Enhancing User Experience Using a Framework Integrating Emotion Recognition and Eye-Tracking}, volume={85}, url={https://drpress.org/ojs/index.php/HSET/article/view/18387}, DOI={10.54097/be97jg10}, abstractNote={
In a rapidly evolving digital landscape, ensuring a seamless and engaging user experience (UX) has become paramount. This study delves into the intricate realm of web interaction design, aiming to enhance user satisfaction and engagement. Through the integration of emotion recognition and eye-tracking technologies, a profound relationship between user emotions and web interface design is unveiled. The central theme of this research revolves around the integration of emotion recognition and eye-tracking technologies to evaluate web interaction designs. An experimental study involving 24 participants from diverse backgrounds and age groups was conducted. These participants navigated web interfaces that featured both emotion recognition and eye-tracking technologies. Three distinct tasks were carefully crafted to represent a spectrum of web interactions, ranging from basic navigation to complex decision-making. Data collection involved real-time valence and arousal scores, video recordings, visual attention patterns, task completion times, and questionnaires. The analysis revealed a series of significant discoveries. Users who engaged with simplified web versions consistently exhibited elevated valence values, indicative of heightened positive emotional feedback, diverging starkly from their counterparts navigating complex web iterations. The dynamic ebb and flow of emotional states, underscored by arousal levels, underscore the pivotal role of real-time emotional assessment in the critical evaluation of web interfaces. Moreover, the study unveiled a persistent preference for a state of emotional calmness during interactions, demonstrating a universal need for user-centered design principles that prioritize minimal cognitive load and emotional tranquility. In summation, this research contributes a robust framework to the design community and academia for the comprehensive evaluation of web interaction designs. The findings underscore the paramount significance of simplicity, real-time emotional evaluation, and unwavering adherence to user-centered design principles in the realm of web interaction. This study constitutes an invaluable repository for designers, developers, and researchers, steering their endeavors towards the relentless pursuit of optimized user experiences within the ever-evolving digital landscape.
}, journal={Highlights in Science, Engineering and Technology}, author={Wang, Jianyi}, year={2024}, month={Mar.}, pages={298–308} }

@misc{lillo2024investigatingrelationshipempathyattribution,
      title={Investigating the relationship between empathy and attribution of mental states to robots}, 
      author={Alberto Lillo and Alessandro Saracco and Elena Siletto and Claudio Mattutino and Cristina Gena},
      year={2024},
      eprint={2405.01019},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2405.01019}, 
}

@article{HAKANPAA2021570,
title = {Comparing Contemporary Commercial and Classical Styles: Emotion Expression in Singing},
journal = {Journal of Voice},
volume = {35},
number = {4},
pages = {570-580},
year = {2021},
issn = {0892-1997},
doi = {https://doi.org/10.1016/j.jvoice.2019.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0892199719302097},
author = {Tua Hakanpää and Teija Waaramaa and Anne-Maria Laukkanen},
keywords = {Emotion expression, Singing voice, Voice quality, Song genre, Acoustic analyses},
abstract = {Summary
Objective
This study examines the acoustic correlates of the vocal expression of emotions in contemporary commercial music (CCM) and classical styles of singing. This information may be useful in improving the training of interpretation in singing.
Study Design
This is an experimental comparative study.
Methods
Eleven female singers with a minimum of 3 years of professional-level singing training in CCM, classical, or both styles participated. They sang the vowel [ɑ:] at three pitches (A3 220Hz, E4 330Hz, and A4 440Hz) expressing anger, sadness, joy, tenderness, and a neutral voice. Vowel samples were analyzed for fundamental frequency (fo) formant frequencies (F1-F5), sound pressure level (SPL), spectral structure (alpha ratio = SPL 1500-5000 Hz—SPL 50-1500 Hz), harmonics-to-noise ratio (HNR), perturbation (jitter, shimmer), onset and offset duration, sustain time, rate and extent of fo variation in vibrato, and rate and extent of amplitude vibrato.
Results
The parameters that were statistically significantly (RM-ANOVA, P ≤ 0.05) related to emotion expression in both genres were SPL, alpha ratio, F1, and HNR. Additionally, for CCM, significance was found in sustain time, jitter, shimmer, F2, and F4. When fo and SPL were set as covariates in the variance analysis, jitter, HNR, and F4 did not show pure dependence on expression. The alpha ratio, F1, F2, shimmer apq5, amplitude vibrato rate, and sustain time of vocalizations had emotion-related variation also independent of fo and SPL in the CCM style, while these parameters were related to fo and SPL in the classical style.
Conclusions
The results differed somewhat for the CCM and classical styles. The alpha ratio showed less variation in the classical style, most likely reflecting the demand for a more stable voice source quality. The alpha ratio, F1, F2, shimmer, amplitude vibrato rate, and the sustain time of the vocalizations were related to fo and SPL control in the classical style. The only common independent sound parameter indicating emotional expression for both styles was SPL. The CCM style offers more freedom for expression-related changes in voice quality.}
}

@article{abbas2024context,
  title = {Context-based Emotion Recognition: A Survey},
  author = {Rizwan Abbas and Bingnan Ni and Ruhui Ma and Teng Li and Yehao Lu and Xi Li},
  journal = {Neurocomputing},
  year = {2024},
  keywords = {emotion recognition, contextual information, emotion detection techniques, real-life applications},
  note = {Preprint submitted to Elsevier, not peer-reviewed},
  url = {https://ssrn.com/abstract=4657124},
  abstract = {Emotions are essential to human communication, and their accurate recognition is crucial for developing intelligent systems capable of effective interaction. This survey explores the critical role of context in emotion recognition, examining techniques, datasets, and applications across fields. It highlights challenges and identifies future directions for advancement in this area.},
}

@misc{liu2024emotiondetectionbodygesture,
      title={Emotion Detection through Body Gesture and Face}, 
      author={Haoyang Liu},
      year={2024},
      eprint={2407.09913},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.09913}, 
}

@Article{computation11030052,
AUTHOR = {Taye, Mohammad Mustafa},
TITLE = {Theoretical Understanding of Convolutional Neural Network: Concepts, Architectures, Applications, Future Directions},
JOURNAL = {Computation},
VOLUME = {11},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {52},
URL = {https://www.mdpi.com/2079-3197/11/3/52},
ISSN = {2079-3197},
ABSTRACT = {Convolutional neural networks (CNNs) are one of the main types of neural networks used for image recognition and classification. CNNs have several uses, some of which are object recognition, image processing, computer vision, and face recognition. Input for convolutional neural networks is provided through images. Convolutional neural networks are used to automatically learn a hierarchy of features that can then be utilized for classification, as opposed to manually creating features. In achieving this, a hierarchy of feature maps is constructed by iteratively convolving the input image with learned filters. Because of the hierarchical method, higher layers can learn more intricate features that are also distortion and translation invariant. The main goals of this study are to help academics understand where there are research gaps and to talk in-depth about CNN’s building blocks, their roles, and other vital issues.},
DOI = {10.3390/computation11030052}
}

@misc{wu2023empiricalstudyimprovementspeech,
      title={An Empirical Study and Improvement for Speech Emotion Recognition}, 
      author={Zhen Wu and Yizhe Lu and Xinyu Dai},
      year={2023},
      eprint={2304.03899},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.03899}, 
}

@misc{kundu2024enhancedspeechemotionrecognition,
      title={Enhanced Speech Emotion Recognition with Efficient Channel Attention Guided Deep CNN-BiLSTM Framework}, 
      author={Niloy Kumar Kundu and Sarah Kobir and Md. Rayhan Ahmed and Tahmina Aktar and Niloya Roy},
      year={2024},
      eprint={2412.10011},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2412.10011}, 
}

@article{DBLP:journals/corr/abs-2002-00269,
  author       = {David Heckerman},
  title        = {A Tutorial on Learning With Bayesian Networks},
  journal      = {CoRR},
  volume       = {abs/2002.00269},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.00269},
  eprinttype    = {arXiv},
  eprint       = {2002.00269},
  timestamp    = {Mon, 10 Feb 2020 15:12:57 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-00269.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{hurst2020socialemotionalskillstraining,
      title={Social and Emotional Skills Training with Embodied Moxie}, 
      author={Nikki Hurst and Caitlyn Clabaugh and Rachel Baynes and Jeff Cohn and Donna Mitroff and Stefan Scherer},
      year={2020},
      eprint={2004.12962},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2004.12962}, 
}

@article{lovot,
author = {Hung, Lillian and Ito, Hiro and Wong, Joey},
year = {2023},
month = {12},
pages = {1077-1077},
title = {LOVOT ROBOT AS COMPANIONS FOR OLDER ADULTS IN LONG-TERM CARE},
volume = {7},
journal = {Innovation in Aging},
doi = {10.1093/geroni/igad104.3460}
}

@inproceedings{wada2006,
  author    = {Kazuyoshi Wada and Takanori Shibata},
  title     = {Robot Therapy in a Care House - Its Sociopsychological and Physiological Effects on the Residents},
  booktitle = {Proceedings of the 2006 IEEE International Conference on Robotics and Automation},
  year      = {2006},
  doi       = {10.1109/ROBOT.2006.1642310},
  url       = {https://doi.org/10.1109/ROBOT.2006.1642310},
  publisher = {IEEE},
  address   = {Orlando, Florida},
}

@misc{picovoice_porcupine,
  author       = {{Picovoice}},
  title        = {Porcupine: Wake word detection engine},
  year         = {2024},
  howpublished = {\url{https://github.com/Picovoice/porcupine}},
  note         = {Accessed: 2025-04-24}
}


@article{han2023,
  author = {Han, Shangfeng and Guo, Yanliang and Zhou, Xinyi and Huang, Junlong and Shen, Linlin and Luo, Yuejia},
  year = {2023},
  month = {12},
  pages = {},
  title = {A Chinese Face Dataset with Dynamic Expressions and Diverse Ages Synthesized by Deep Learning},
  volume = {10},
  journal = {Scientific Data},
  doi = {10.1038/s41597-023-02701-2}
}

@misc{simonyan2015deepconvolutionalnetworkslargescale,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.1556}, 
}

@misc{vistec_ai_ser,
  author       = {{vistec-AI}},
  title        = {VISTEC-SER: Speech Emotion Recognition Toolkit},
  year         = {2024},
  howpublished = {\url{https://github.com/vistec-AI/vistec-ser}},
  note         = {Accessed: 2025-04-24}
}

@article{10.1371/journal.pone.0145450,
    doi = {10.1371/journal.pone.0145450},
    author = {Trampe, Debra AND Quoidbach, Jordi AND Taquet, Maxime},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Emotions in Everyday Life},
    year = {2015},
    month = {12},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0145450},
    pages = {1-15},
    number = {12},
}

@article{Olaru_2016,
doi = {10.1088/1757-899X/147/1/012027},
url = {https://dx.doi.org/10.1088/1757-899X/147/1/012027},
year = {2016},
month = {aug},
publisher = {IOP Publishing},
volume = {147},
number = {1},
pages = {012027},
author = {Olaru, D and Balan, M R and Tufescu, A},
title = {Influence of the cage on friction torque in low loaded thrust ball bearing operating in dry conditions},
journal = {IOP Conference Series: Materials Science and Engineering},
abstract = {The authors investigated analytically and experimentally the friction torque in a modified thrust ball bearing operating at very low axial load in dry conditions by using only three balls and a cage. The experiments were conducted by using spin-down methodology. The results evidenced the influence of the sliding friction between the cage and the balls on the total friction torque. It was concluded that at very low loads the friction between cage and balls in a thrust ball bearing has an important contribution on total friction torque.}
}
